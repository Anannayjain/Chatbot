# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1viyhL0x4LOLwRysjkVAkIEDkAPytGP3b
"""

!pip install langchain
!pip install huggingface_hub
!pip install sentence_transformers

pip install unstructured

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

import requests
from bs4 import BeautifulSoup

def get_sitemap_urls(sitemap_url):
    response = requests.get(sitemap_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'xml')
        return [loc.text for loc in soup.find_all('loc')]
    else:
        print(f"Failed to fetch sitemap. Status code: {response.status_code}")
        return []

# Replace 'your_sitemap_url' with the actual sitemap URL of the website
sitemap_url = 'https://ee-damp.github.io/sitemap.xml'
urls = get_sitemap_urls(sitemap_url)

print("List of URLs from the sitemap:")
ls=[]
for url in urls:
    ls.append(url)

len(ls)

urls =ls[:200]

urls

from urllib.parse import urlparse

urls[100]

from langchain.document_loaders import UnstructuredURLLoader
loaders = UnstructuredURLLoader(urls=urls)
data = loaders.load()

len(data)

# Text Splitter
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(separator='\n',
                                      chunk_size=800,
                                      chunk_overlap=200)


docs = text_splitter.split_documents(data)

docs

len(docs)

# Embeddings
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings()

!pip install faiss-cpu

# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html
from langchain.vectorstores import FAISS

db = FAISS.from_documents(docs, embeddings)

import pickle
# Save the index to a file using pickle
index_filename = 'ind.pkl'
with open(index_filename, 'wb') as file:
    pickle.dump(db, file)

# Save the index to a file
# Later, when you want to use the index again, you can load it
# with open(index_filename, 'rb') as file:
#     loaded_db = pickle.load(file)

# Later, when you want to use the index again, you can load it
# loaded_db = FAISS.load(index_filename)

query = "Tell me about EE309"
docs = db.similarity_search(query)

import textwrap

def wrap_text_preserve_newlines(text, width=110):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text

print(wrap_text_preserve_newlines(str(docs[0].page_content)))

from langchain.chains.question_answering import load_qa_chain
from langchain import HuggingFaceHub

llm=HuggingFaceHub(repo_id="google/flan-t5-xxl", model_kwargs={"temperature":0.5, "max_length":512})

chain = load_qa_chain(llm, chain_type="stuff")

query = "interview "
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "list of courses in CS minor"
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "list me elective coursed of communications"
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "course by prof virendra singh?"
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query)

